@software{chaseLangChain2022,
  title = {{{LangChain}}},
  author = {Chase, Harrison},
  date = {2022-10},
  origdate = {2022-10-17T02:58:36Z},
  url = {https://github.com/langchain-ai/langchain},
  urldate = {2023-12-12},
  abstract = {⚡ Building applications with LLMs through composability ⚡}
}

@online{holzingerWhatWeNeed2017,
  title = {What Do We Need to Build Explainable {{AI}} Systems for the Medical Domain?},
  author = {Holzinger, Andreas and Biemann, Chris and Pattichis, Constantinos S. and Kell, Douglas B.},
  date = {2017-12-28},
  eprint = {1712.09923},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1712.09923},
  url = {http://arxiv.org/abs/1712.09923},
  urldate = {2023-11-22},
  abstract = {Artificial intelligence (AI) generally and machine learning (ML) specifically demonstrate impressive practical success in many different application domains, e.g. in autonomous driving, speech recognition, or recommender systems. Deep learning approaches, trained on extremely large data sets or using reinforcement learning methods have even exceeded human performance in visual tasks, particularly on playing games such as Atari, or mastering the game of Go. Even in the medical domain there are remarkable results. The central problem of such models is that they are regarded as black-box models and even if we understand the underlying mathematical principles, they lack an explicit declarative knowledge representation, hence have difficulty in generating the underlying explanatory structures. This calls for systems enabling to make decisions transparent, understandable and explainable. A huge motivation for our approach are rising legal and privacy aspects. The new European General Data Protection Regulation entering into force on May 25th 2018, will make black-box approaches difficult to use in business. This does not imply a ban on automatic learning approaches or an obligation to explain everything all the time, however, there must be a possibility to make the results re-traceable on demand. In this paper we outline some of our research topics in the context of the relatively new area of explainable-AI with a focus on the application in medicine, which is a very special domain. This is due to the fact that medical professionals are working mostly with distributed heterogeneous and complex sources of data. In this paper we concentrate on three sources: images, *omics data and text. We argue that research in explainable-AI would generally help to facilitate the implementation of AI/ML in the medical domain, and specifically help to facilitate transparency and trust.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Statistics - Machine Learning}
}

@article{johnson2019billion,
  title = {Billion-Scale Similarity Search with {{GPUs}}},
  author = {Johnson, Jeff and Douze, Matthijs and Jégou, Hervé},
  date = {2019},
  journaltitle = {IEEE Transactions on Big Data},
  volume = {7},
  number = {3},
  pages = {535--547},
  publisher = {{IEEE}}
}

@misc{krausEXPLAINABLEAIRequirements2022,
  title = {{{EXPLAINABLE AI}}: {{Requirements}}, {{Use Cases}} and {{Solutions}}},
  author = {Kraus, Dr. Thomas and Ganschow, Lene and Eisenträger, Marlene and Wischmann, Dr. Steffen},
  editor = {{Bundesministerium für Wirtschaft und Klimaschutz}},
  date = {2022-04},
  langid = {english}
}

@software{leungRunningLlamaOther2023,
  title = {Running {{Llama}} 2 and Other {{Open-Source LLMs}} on {{CPU Inference Locally}} for {{Document Q}}\&{{A}}},
  author = {Leung, Kenneth},
  date = {2023-11-26T02:09:02Z},
  origdate = {2023-07-06T05:42:43Z},
  url = {https://github.com/kennethleungty/Llama-2-Open-Source-LLM-CPU-Inference},
  urldate = {2023-11-28},
  abstract = {Running Llama 2 and other Open-Source LLMs on CPU Inference Locally for Document~Q\&A},
  keywords = {c-transformers,chatgpt,cpu,cpu-inference,deep-learning,document-qa,faiss,langchain,language-models,large-language-models,llama,llama-2,llm,machine-learning,natural-language-processing,nlp,open-source-llm,python,sentence-transformers,transformers}
}

@online{vanakenClinicalOutcomePrediction2021,
  title = {Clinical {{Outcome Prediction}} from {{Admission Notes}} Using {{Self-Supervised Knowledge Integration}}},
  author = {family=Aken, given=Betty, prefix=van, useprefix=true and Papaioannou, Jens-Michalis and Mayrdorfer, Manuel and Budde, Klemens and Gers, Felix A. and Löser, Alexander},
  date = {2021-02-08},
  eprint = {2102.04110},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2102.04110},
  urldate = {2023-11-22},
  abstract = {Outcome prediction from clinical text can prevent doctors from overlooking possible risks and help hospitals to plan capacities. We simulate patients at admission time, when decision support can be especially valuable, and contribute a novel admission to discharge task with four common outcome prediction targets: Diagnoses at discharge, procedures performed, in-hospital mortality and length-of-stay prediction. The ideal system should infer outcomes based on symptoms, pre-conditions and risk factors of a patient. We evaluate the effectiveness of language models to handle this scenario and propose clinical outcome pre-training to integrate knowledge about patient outcomes from multiple public sources. We further present a simple method to incorporate ICD code hierarchy into the models. We show that our approach improves performance on the outcome tasks against several baselines. A detailed analysis reveals further strengths of the model, including transferability, but also weaknesses such as handling of vital values and inconsistencies in the underlying data.},
  pubstate = {preprint},
  version = {1},
  keywords = {Computer Science - Computation and Language}
}
